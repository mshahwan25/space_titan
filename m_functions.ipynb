{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc491358",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b876f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from collections import Counter #for IQR method\n",
    "from scipy.stats import median_abs_deviation #for modified z-score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.feature_selection import f_regression, VarianceThreshold, SelectKBest, SelectPercentile, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8cf970",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92becfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from collections import Counter #for IQR method\n",
    "from scipy.stats import median_abs_deviation #for modified z-score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.feature_selection import f_regression, VarianceThreshold, SelectKBest, SelectPercentile, chi2, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\"\"\"\n",
    "\n",
    "#calculate adjusted r2\n",
    "def m_adjusted_r2(obs_num, var_num,r2):\n",
    "    return (1-(1-r2)*((obs_num-1)/(obs_num-var_num-1)))\n",
    "\n",
    "#show features regression in Dataframe\n",
    "def m_fregression(x,y):\n",
    "    features_regression = pd.DataFrame(columns=['feature','coefficient','pval'])\n",
    "    for i in range (x.shape[1]):\n",
    "        df_f_regression = f_regression(x,y)\n",
    "        row = {'feature':x.columns[i],'coefficient':df_f_regression[0][i].round(2),'pval':df_f_regression[1][i].round(5)}\n",
    "        features_regression = features_regression.append(row,ignore_index=True)\n",
    "    return (features_regression)\n",
    "\n",
    "#calculating multicolinearity between variables\n",
    "def m_VIF(x,y):\n",
    "    #get columns names\n",
    "    a = x.columns\n",
    "    b = y.columns\n",
    "    \n",
    "    #loop to generate a (specially-formated) string containing (dependant variable) and (independent variables)\n",
    "    string_fun = '{}~'.format(b[0])\n",
    "    for i in range(0,len(a),1):\n",
    "        string_fun = string_fun+\"{}+\".format(a[i])\n",
    "        \n",
    "    #to drop the last (+)\n",
    "    string_fun= string_fun[0:len(string_fun)-1]\n",
    "    string_fun\n",
    "    \n",
    "    #generate a full dataframe containing dependent and independent variables\n",
    "    df_vif_gen = pd.merge(left=y,right=x,left_index=True,right_index=True)\n",
    "    \n",
    "    \n",
    "    #find design matrix for regression model using 'rating' as response variable \n",
    "    y,x = dmatrices(string_fun, data=df_vif_gen, return_type='dataframe')\n",
    "\n",
    "    #create DataFrame to hold VIF values\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df['variable'] = x.columns \n",
    "\n",
    "    #calculate VIF for each predictor variable \n",
    "    vif_df['VIF'] = np.round([variance_inflation_factor(x.values, i) for i in range(x.shape[1])],2)\n",
    "\n",
    "    #view VIF for each predictor variable \n",
    "    print ('VIF=1: There is no correlation between a given predictor variable and any other predictor variables in the model.\\n')\n",
    "    print ('VIF=(1-5): There is moderate correlation between a given predictor variable and other predictor variables in the model.\\n')\n",
    "    print ('VIF>5: There is severe correlation between a given predictor variable and other predictor variables in the model.')\n",
    "\n",
    "    return vif_df\n",
    "\n",
    "# find correlated features\n",
    "def m_correlation(dataset, threshold):\n",
    "    col_corr = []  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.append(colname)\n",
    "    return col_corr\n",
    "\n",
    "#reduce dataframe by quantile value (outlier), return the reduced dataframe and print the curve\n",
    "def m_df_trim (dataframe,column_name,q_low,q_high):   \n",
    "    q_low = dataframe[column_name].quantile(q=q_low)\n",
    "    q_high = dataframe[column_name].quantile(q=q_high)\n",
    "\n",
    "    df_reduced = dataframe[(dataframe[column_name]>q_low)&(dataframe[column_name]<=q_high)]\n",
    "    print (np.round(100-df_reduced.shape[0]/dataframe.shape[0]*100,2),\"% of data will be lost\")\n",
    "    fig, ax = plt.subplots(figsize=(15,5),nrows=1,ncols=2);\n",
    "    sns.histplot(data=dataframe,x=column_name,ax=ax[0],label='Original');\n",
    "    sns.histplot(data=df_reduced,x=column_name,ax=ax[1],label='Reduced')\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    return df_reduced\n",
    "\n",
    "#creating different linear regression models and a dataframe containing the summary info\n",
    "def m_mlinear_regression(xtr,xts,ytr,yts):\n",
    "#create a dataframe for modeling summary \n",
    "    models_summary = pd.DataFrame(columns=['Model','Type','Scaled','R2-Score','RMSE-Score'])\n",
    "\n",
    "    #multiple linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='Linear'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    \n",
    "    #support vector regression\n",
    "    model_variables=['rbf','linear','poly','sigmoid']\n",
    "    for i in model_variables:\n",
    "        model = SVR(kernel=i)\n",
    "        model.fit(xtr,ytr)\n",
    "        ypr = model.predict(xts)\n",
    "        #\n",
    "        model_name='SVR'\n",
    "        model_type=i\n",
    "        scaled='No'\n",
    "        score = r2_score(yts,ypr)\n",
    "        score2=mean_squared_error(yts,ypr,squared=False)\n",
    "        #plt.scatter(yts,ypr);\n",
    "        #plt.plot(yts,ypr,color='r');\n",
    "        models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Decision treeregression\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='Decision tree'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Random forest regression\n",
    "    model_variables = [100,200,300,400,500]\n",
    "    for i in model_variables:\n",
    "        model = RandomForestRegressor(n_estimators=i)\n",
    "        model.fit(xtr,ytr)\n",
    "        ypr = model.predict(xts)\n",
    "        #\n",
    "        model_name='Random forest'\n",
    "        model_type=i\n",
    "        scaled='No'\n",
    "        score = r2_score(yts,ypr)\n",
    "        score2=mean_squared_error(yts,ypr,squared=False)\n",
    "        #plt.scatter(yts,ypr);\n",
    "        #plt.plot(yts,ypr,color='r');\n",
    "        models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #XGBoost\n",
    "    model = XGBRegressor()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='XGBoost'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    \n",
    "    #Lasso\n",
    "    model = Lasso()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='Lasso'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Ridge\n",
    "    model = Ridge()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='Ridge'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #KernelRidge\n",
    "    model = KernelRidge()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='KernelRidge'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #BayesianRidge\n",
    "    model = BayesianRidge()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='BayesianRidge'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #ElasticNet\n",
    "    model = ElasticNet()\n",
    "    model.fit(xtr,ytr)\n",
    "    ypr = model.predict(xts)\n",
    "    #\n",
    "    model_name='ElasticNet'\n",
    "    model_type='General'\n",
    "    scaled='No'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "#---------------SCALING-------------------#\n",
    "    sc    = StandardScaler()              #\n",
    "    xtr_sc= sc.fit_transform(xtr)         #\n",
    "    xts_sc= sc.transform(xts)             #\n",
    "#-----------------------------------------#\n",
    "    #multiple linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='Linear'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    \n",
    "    #support vector regression\n",
    "    model_variables=['rbf','linear','poly','sigmoid']\n",
    "    for i in model_variables:\n",
    "        model = SVR(kernel=i)\n",
    "        model.fit(xtr_sc,ytr)\n",
    "        ypr = model.predict(xts_sc)\n",
    "        #\n",
    "        model_name='SVR'\n",
    "        model_type=i\n",
    "        scaled='Yes'\n",
    "        score = r2_score(yts,ypr)\n",
    "        score2=mean_squared_error(yts,ypr,squared=False)\n",
    "        #plt.scatter(yts,ypr);\n",
    "        #plt.plot(yts,ypr,color='r');\n",
    "        models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Decision treeregression\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='Decision tree'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Random forest regression\n",
    "    model_variables = [100,200,300,400,500]\n",
    "    for i in model_variables:\n",
    "        model = RandomForestRegressor(n_estimators=i)\n",
    "        model.fit(xtr_sc,ytr)\n",
    "        ypr = model.predict(xts_sc)\n",
    "        #\n",
    "        model_name='Random forest'\n",
    "        model_type=i\n",
    "        scaled='Yes'\n",
    "        score = r2_score(yts,ypr)\n",
    "        score2=mean_squared_error(yts,ypr,squared=False)\n",
    "        #plt.scatter(yts,ypr);\n",
    "        #plt.plot(yts,ypr,color='r');\n",
    "        models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #XGBoost\n",
    "    model = XGBRegressor()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='XGBoost'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "\n",
    "    #Lasso\n",
    "    model = Lasso()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='Lasso'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    \n",
    "    #Ridge\n",
    "    model = Ridge()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='Ridge'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    \n",
    "    #KernelRidge\n",
    "    model = KernelRidge()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='KernelRidge'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    \n",
    "    #BayesianRidge\n",
    "    model = BayesianRidge()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='BayesianRidge'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    \n",
    "    #ElasticNet\n",
    "    model = ElasticNet()\n",
    "    model.fit(xtr_sc,ytr)\n",
    "    ypr = model.predict(xts_sc)\n",
    "    #\n",
    "    model_name='ElasticNet'\n",
    "    model_type='General'\n",
    "    scaled='Yes'\n",
    "    score = r2_score(yts,ypr)\n",
    "    score2=mean_squared_error(yts,ypr,squared=False)\n",
    "    #plt.scatter(yts,ypr);\n",
    "    #plt.plot(yts,ypr,color='r');\n",
    "    models_summary = models_summary.append({'Model':model_name,'Type':model_type,'Scaled':scaled,'R2-Score':score,'RMSE-Score':score2},ignore_index=True)\n",
    "    models_summary['R2-Score']=models_summary['R2-Score'].round(3)\n",
    "    models_summary['RMSE-Score']=models_summary['RMSE-Score'].round(3)\n",
    "            \n",
    "    model_summary = models_summary.sort_values(by=['RMSE-Score'],ascending=True)\n",
    "    model_summary.reset_index(inplace=True)\n",
    "    return model_summary\n",
    "\n",
    "#return a list and graph of how features importance to target value\n",
    "def m_feature_importance (xtrain,ytrain,n_estimators):\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    df_features_importance = pd.DataFrame({'features':xtrain.columns,'importance':rf.feature_importances_})\n",
    "    df_features_importance.sort_values(by='importance',axis=0,inplace=True,ascending=False)\n",
    "    sns.barplot(x=df_features_importance['importance'],\n",
    "                y=df_features_importance['features'],\n",
    "                color='b')  \n",
    "    return df_features_importance\n",
    "\n",
    "#list the features which have correlation of specific values against the rest of dataset features\n",
    "def m_corr_list(df,min_up=0.6,min_down=-0.6):\n",
    "    corr_mat=df.corr()\n",
    "#loop through orrelation-matrix rows\n",
    "    for x in corr_mat.index:\n",
    "        #list for features having positive correlation against first feature\n",
    "        corr_feat_up=[]\n",
    "        #list for features having negative correlation against first feature\n",
    "        corr_feat_down=[]\n",
    "        \n",
    "        #add features which have correlation in the specified limits\n",
    "        for y in corr_mat.columns:\n",
    "            if (corr_mat.loc[x,y]>=min_up) and (x!=y): #skip feature against same feature\n",
    "                corr_feat_up.append(y)\n",
    "            elif (corr_mat.loc[x,y]<=min_down) and (x!=y): #skip feature against same feature\n",
    "                corr_feat_down.append(y)\n",
    "        if len(corr_feat_up)!=0:\n",
    "            print ('\\033[1m'+x+'\\033[0m'+' is positively correlated with {}'.format(corr_feat_up))\n",
    "        if len(corr_feat_down)!=0:\n",
    "            print ('\\033[1m'+x+'\\033[0m'+' is negatively correlated with {}'.format(corr_feat_down))\n",
    "        if len(corr_feat_up)!=0 or len(corr_feat_down)!=0:\n",
    "            print ('')           \n",
    "\n",
    "#return a list and graph of how features importance to target value\n",
    "def m_feature_importance (xtrain,ytrain,n_estimators):\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    df_features_importance = pd.DataFrame({'features':xtrain.columns,'importance':rf.feature_importances_})\n",
    "    df_features_importance.sort_values(by='importance',axis=0,inplace=True,ascending=False)\n",
    "    sns.barplot(x=df_features_importance['importance'],\n",
    "                y=df_features_importance['features'],\n",
    "                color='b')  \n",
    "    return df_features_importance\n",
    "\n",
    "#split dataframe into target variable and features\n",
    "def m_xy_split(df,target_column):\n",
    "    col = (df.columns).drop(target_column)\n",
    "    y = df[[target_column]]\n",
    "    x = df[col]\n",
    "    return x,y\n",
    "\n",
    "def m_scale_encode_split(dftrain,target_feature,num_feature,cat_feature,train_size=0.85,dftest=None):\n",
    "    #separating target variable and features\n",
    "    df=dftrain\n",
    "    y_ml=df[target_feature]\n",
    "    x_ml=df[num_feature+cat_feature]\n",
    "\n",
    "    #Encoding categorical features\n",
    "    x_ml_dum = pd.get_dummies(x_ml,columns=cat_feature,drop_first=True)\n",
    "\n",
    "    #splitting data\n",
    "    xtr,xts,ytr,yts=train_test_split(x_ml_dum,y_ml,train_size=train_size)\n",
    "\n",
    "    #Scaling numerical features\n",
    "    xtr_sc=xtr.copy()\n",
    "    xts_sc=xts.copy()\n",
    "    sc=StandardScaler()\n",
    "    xtr_sc[num_feature]=sc.fit_transform(xtr[num_feature])\n",
    "    xts_sc[num_feature]=sc.transform(xts[num_feature])\n",
    "    \n",
    "    \n",
    "    if dftest is None:\n",
    "        return xtr_sc,xts_sc,ytr,yts\n",
    "    else:\n",
    "        dfts=dftest[num_feature+cat_feature]\n",
    "        dfts_dum = pd.get_dummies(dfts,columns=cat_feature,drop_first=True)\n",
    "        dfts_sc=dfts_dum.copy()\n",
    "        dfts_sc[num_feature]=sc.transform(dfts_sc[num_feature])\n",
    "        \n",
    "        return xtr_sc,xts_sc,ytr,yts,dfts_sc\n",
    "\n",
    "#from collections import Counter #for IQR method\n",
    "def m_outlier_iqr (df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and returns an index list corresponding to the observations \n",
    "    containing more than n outliers according to the Tukey IQR method.\n",
    "    \"\"\"\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "                \n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[column], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[column],75)\n",
    "        \n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determining a list of indices of outliers\n",
    "        outlier_list_column = df[(df[column] < Q1 - outlier_step) | (df[column] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # appending the list of outliers \n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # selecting observations containing more than x outliers\n",
    "    outlier_list = Counter(outlier_list)        \n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
    "    \n",
    "    # Calculate the number of records below and above lower and above bound value respectively\n",
    "    df1 = df[df[column] < Q1 - outlier_step]\n",
    "    df2 = df[df[column] > Q3 + outlier_step]\n",
    "    \n",
    "    print('Total number of outliers is:', df1.shape[0]+df2.shape[0])\n",
    "    print('Total number of observations containing more than {} outliers is: {} '.format(n,len(multiple_outliers)))\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(len(multiple_outliers)/df.shape[0]*100,0)))\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "def m_outlier_std (df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns an index list corresponding to the observations \n",
    "    containing more than n outliers according to the standard deviation method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    for column in features:\n",
    "        # calculate the mean and standard deviation of the data frame\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        \n",
    "        # calculate the cutoff value\n",
    "        cut_off = data_std * 3\n",
    "        \n",
    "        # Determining a list of indices of outliers for feature column        \n",
    "        outlier_list_column = df[(df[column] < data_mean - cut_off) | (df[column] > data_mean + cut_off)].index\n",
    "        \n",
    "        # appending the found outlier indices for column to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_column)\n",
    "        \n",
    "    # selecting observations containing more than x outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    # Calculate the number of records below and above lower and above bound value respectively\n",
    "    df1 = df[df[column] > data_mean + cut_off]\n",
    "    df2 = df[df[column] < data_mean - cut_off]\n",
    "    print('Total number of outliers is:', df1.shape[0]+ df2.shape[0])\n",
    "    print('Total number of observations containing more than {} outliers is: {} '.format(n,len(multiple_outliers)))\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(len(multiple_outliers)/df.shape[0]*100,0)))\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "#from scipy.stats import median_abs_deviation #for modified z-score\n",
    "def m_outlier_zscore (df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns an index list corresponding to the observations \n",
    "    containing more than n outliers according to the z-score method.\n",
    "    \"\"\"\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "        # calculate the mean and standard deviation of the data frame\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        threshold = 3\n",
    "        \n",
    "        z_score = abs( (df[column] - data_mean)/data_std )\n",
    "        \n",
    "        # Determining a list of indices of outliers for feature column        \n",
    "        outlier_list_column =  df[z_score > threshold].index\n",
    "        \n",
    "        # appending the found outlier indices for column to the list of outlier indices \n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # selecting observations containing more than x outliers\n",
    "    outlier_list = Counter(outlier_list)        \n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
    "    \n",
    "    # Calculate the number of outlier records\n",
    "    df1 = df[z_score > threshold]\n",
    "    print('Total number of outliers is:', df1.shape[0])\n",
    "    print('Total number of observations containing more than {} outliers is: {} '.format(n,len(multiple_outliers)))\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(len(multiple_outliers)/df.shape[0]*100,0)))\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "def m_outlier_zscore_modified (df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns an index list corresponding to the observations \n",
    "    containing more than n outliers according to the z-score modified method.\n",
    "    \"\"\"\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "        # calculate the mean and standard deviation of the data frame\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        threshold = 3\n",
    "        MAD = median_abs_deviation\n",
    "        \n",
    "        mod_z_score = abs(0.6745*(df[column] - data_mean)/MAD(df[column]) )\n",
    "                \n",
    "        # Determining a list of indices of outliers for feature column        \n",
    "        outlier_list_column =  df[mod_z_score >threshold].index\n",
    "        \n",
    "        # appending the found outlier indices for column to the list of outlier indices \n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # selecting observations containing more than x outliers\n",
    "    outlier_list = Counter(outlier_list)        \n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
    "    \n",
    "    # Calculate the number of outlier records\n",
    "    df1 = df[mod_z_score >threshold]\n",
    "    print('Total number of outliers is:', df1.shape[0])\n",
    "    print('Total number of observations containing more than {} outliers is: {} '.format(n,len(multiple_outliers)))\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(len(multiple_outliers)/df.shape[0]*100,0)))\n",
    "    \n",
    "    return multiple_outliers\n",
    "\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "def m_outlier_isolationforest(df,features=[]):\n",
    "    df=df[features]\n",
    "    outlier_model = IsolationForest()\n",
    "    outlier_model.fit(df)\n",
    "    anomaly = outlier_model.predict(df)\n",
    "    df['anomaly'] = anomaly\n",
    "    print ('The total number of outliers is: ',df[(df['anomaly']==-1)]['anomaly'].count())\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(df[(df['anomaly']==-1)]['anomaly'].count()/df.shape[0]*100,0)))\n",
    "\n",
    "    return df\n",
    "\n",
    "#from sklearn.cluster import DBSCAN\n",
    "def m_outlier_DBSCAN(df,features=[]):\n",
    "    df=df[features]\n",
    "    sc = StandardScaler()\n",
    "    df_sc = sc.fit_transform(df)\n",
    "    model_DBSCAN = DBSCAN()\n",
    "    model_DBSCAN.fit(df_sc)\n",
    "    labels = model_DBSCAN.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    labels = pd.Series(labels)\n",
    "    print ('The total number of clusters without outliers: ',n_clusters)\n",
    "    print ('The number of outliers is: ',labels[(labels==-1)].count())\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(labels[(labels==-1)].count()/df.shape[0]*100,0)))           \n",
    "    df['label']=labels\n",
    "    return df\n",
    "\n",
    "#perfrom all outliers detection methods\n",
    "def m_outlier_all (df,n,features=[]):\n",
    "    \n",
    "    print (\"Method IQR:\")\n",
    "    out_iqr = m_outlier_iqr(df,n=n,features=features)\n",
    "    print (\"\\nMethod Standard deviation:\")\n",
    "    out_std = m_outlier_std(df,n=n,features=features)\n",
    "    print (\"\\nMethod Z-Score:\")\n",
    "    out_zsc = m_outlier_zscore(df,n=n,features=features)\n",
    "    print (\"\\nMethod Modified Z-Score:\")\n",
    "    out_mzs = m_outlier_zscore_modified(df,n=n,features=features)\n",
    "    print (\"\\nMethod Isolation forest:\")\n",
    "    df_isf = m_outlier_isolationforest(df,features=features)\n",
    "    out_isf = list(df_isf[(df_isf['anomaly']==-1)].index)\n",
    "    print (\"\\nMethod DBSCAN:\")\n",
    "    df_dbscan = m_outlier_DBSCAN(df,features=features)\n",
    "    out_dbs = list(df_dbscan[(df_dbscan['label']==-1)].index)\n",
    "    \n",
    "    out_total = set(out_iqr).intersection(out_std,out_zsc,out_mzs,out_dbs,out_isf)\n",
    "    print (100*'-')\n",
    "    print('Total number of outliers is: {} '.format(len(out_total)))\n",
    "    print('Percentage of data to be lost is: %{}'.format(np.round(len(out_total)/df.shape[0]*100,0)))\n",
    "    \n",
    "    return out_total\n",
    "    \n",
    "#Fast EDA\n",
    "class m_describe:\n",
    "    def __init__(self,dataframe):\n",
    "        self.dataframe=df\n",
    "\n",
    "    def show_main (df,export_notes=False):\n",
    "        df=df\n",
    "        print (\"Total number of features: \", len(df.columns))\n",
    "        print (\"Total number of observations: \", len(df))\n",
    "        print ('-'*70)\n",
    "        print ('Total number of numerical values: ',len(list(df.select_dtypes(include=['int64', 'float64']).columns)))\n",
    "        print ('Total number of categorical values: ',len(list(df.select_dtypes(include=['object']).columns)))\n",
    "        print ('-'*70)\n",
    "        print (\"Total number of missing values: \", df.isnull().sum().sum())\n",
    "        print (\"Total number of duplicate rows: \", df.duplicated().sum())\n",
    "        plt.pie(x=[len(list(df.select_dtypes(include=['int64', 'float64']).columns)),\n",
    "                   len(list(df.select_dtypes(include=['object']).columns))],\n",
    "               labels=['Numerical Features','Categorical Features'],autopct='%1.0f%%')\n",
    "        plt.title('Features Types')\n",
    "             \n",
    "        #export features_notes \n",
    "        if export_notes==True:\n",
    "            x= list(df.select_dtypes(include=['object']).columns) #get list of numerical features\n",
    "            cat_df = pd.DataFrame(x, columns=['Feature']) #create dataframe containg numerical features\n",
    "            cat_df['Type']='Categorical'\n",
    "\n",
    "            y= list(df.select_dtypes(include=['int64', 'float64']).columns) #get list of categorical features\n",
    "            num_df = pd.DataFrame(y, columns=['Feature']) #create dataframe containing categorical features\n",
    "            num_df['Type']='Numerical'\n",
    "\n",
    "            fea_df = pd.concat([num_df, cat_df]) #merge above dataframes\n",
    "            fea_df[['Description','Importance','Observation','To Do']]=\"\"\n",
    "            fea_df.to_excel('notes.xlsx',index=False)\n",
    "            del x,y,fea_df\n",
    "\n",
    "    \n",
    "    #returns a list of numerical values\n",
    "    def num(df):\n",
    "        df=df\n",
    "        return list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "    #returns a list of categorical values\n",
    "    def cat(df):\n",
    "        df=df\n",
    "        return list(df.select_dtypes(include=['object']).columns)\n",
    "    \n",
    "    #returns a list of missing values sorted from high to low and categorized by features \n",
    "    def missing_val_list(df):\n",
    "        df=df\n",
    "        x= list(df.select_dtypes(include=['object']).columns) #get list of numerical features\n",
    "        cat_df = pd.DataFrame(x, columns=['Feature']) #create dataframe containg numerical features\n",
    "        cat_df['Type']='Categorical'\n",
    "\n",
    "        y= list(df.select_dtypes(include=['int64', 'float64']).columns) #get list of categorical features\n",
    "        num_df = pd.DataFrame(y, columns=['Feature']) #create dataframe containing categorical features\n",
    "        num_df['Type']='Numerical'\n",
    "\n",
    "        fea_df = pd.concat([num_df, cat_df]) #merge above dataframes\n",
    "\n",
    "        x= list(df.columns) #get list of all features\n",
    "        y= list(np.round(df.isnull().sum()/len(df)*100,0)) #get % of null values of all features\n",
    "        z= list((df.isnull().sum())) #get sum of null values of all features\n",
    "        nul_df = pd.DataFrame({'Feature':x,'count of nulls':z,'null % of total obesrvations':y}) #create dataframe from x,y,z\n",
    "\n",
    "        nul_df = pd.merge(fea_df,nul_df,on='Feature',how='outer') #merge all dataframes\n",
    "        nul_df=nul_df[(nul_df['count of nulls']>0)] #show only features having null values\n",
    "        nul_df.sort_values(by=['Type','null % of total obesrvations'],ascending=False) #sorting\n",
    "        nul_df=nul_df.reset_index(drop=True) #drop index\n",
    "        del x,y,z,cat_df,num_df #delete all temporary variables\n",
    "        return nul_df \n",
    "    \n",
    "    #visualize numerical feauters\n",
    "    def visualize_num(df,\n",
    "                      features='all',\n",
    "                     figure_size=(15,150),color=sns.color_palette('Set2')[0]):\n",
    "        df=df\n",
    "        if features == 'all':\n",
    "            features=list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "        fig, a=plt.subplots(nrows=len(features),ncols=2,figsize=figure_size)\n",
    "        for i in range(len(features)):\n",
    "            sns.histplot(data=df[features[i]],color=color,ax=a[i,0])\n",
    "            sns.boxplot(y=df[features[i]],color=color,ax=a[i,1])\n",
    "        del features\n",
    "        #return fig\n",
    "        \n",
    "    #visualize categorical features\n",
    "    def visualize_cat(df,\n",
    "                      features='all',\n",
    "                      figure_size=(15,150), color_scheme='Set2'):\n",
    "        df=df\n",
    "        if features == 'all':\n",
    "            features=list(df.select_dtypes(include=['object']).columns)\n",
    "        fig, a=plt.subplots(nrows=len(features),ncols=2,figsize=figure_size)\n",
    "        for i in range (len(features)):\n",
    "            sorted_labels=df[features[i]].value_counts().sort_values(ascending=False).index\n",
    "            sns.countplot(df[features[i]],palette=color_scheme,order=sorted_labels,ax=a[i,0])\n",
    "            a[i,1].pie(df[features[i]].value_counts(),labels=list(sorted_labels),autopct='%1.0f%%',colors=sns.color_palette(sns.color_palette(color_scheme)));\n",
    "        del features, sorted_labels\n",
    "        #return fig\n",
    "    \n",
    "    #show first \n",
    "    def show_category_concentraction(df,min_percent=0.95):\n",
    "        #create auxilary dataframe\n",
    "        x=pd.DataFrame(df.nunique(),columns=['Unique values'])\n",
    "        x=x.reset_index()\n",
    "        #add new column -feature\n",
    "        x=x.rename(columns={'index':'Feature'})\n",
    "        x= x.sort_values(by='Unique values',ascending=True)\n",
    "\n",
    "        #get list of features\n",
    "        list_of_features=list(x['Feature'].unique())\n",
    "        #loop through all feature and get percentage of data in the top category\n",
    "        for i in range(len(list_of_features)):\n",
    "            feature=list_of_features[i]\n",
    "            n_unique = df[feature].nunique()\n",
    "            value_counts = df[feature].value_counts()\n",
    "            percentage=np.round(value_counts.max() / value_counts.sum()*100)\n",
    "            x.loc[x['Feature']==feature, '1st category concentration -%'] = percentage\n",
    "        x=x[x['1st category concentration -%']/100>=min_percent]\n",
    "        x = x.sort_values(by=['Unique values','1st category concentration -%'],ascending=[True,False])\n",
    "        x=x.reset_index(drop=True)\n",
    "        return x\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "477.333px",
    "left": "150px",
    "top": "110.805px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
